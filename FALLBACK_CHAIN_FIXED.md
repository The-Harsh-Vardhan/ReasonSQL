# LLM Fallback Chain - Fixed Implementation

## Summary

Fixed unsafe and incorrect LLM fallback chain to implement a **deterministic, demo-safe** provider sequence with graceful quota exhaustion handling.

---

## Changes Made

### 1. **Single Source of Truth for Providers** ‚úÖ

**File:** `config/settings.py`

```python
# Provider configuration (SINGLE SOURCE OF TRUTH)
PRIMARY_PROVIDER = "gemini"  # Always Gemini first
SECONDARY_PROVIDER = "groq"  # Groq as fallback
TERTIARY_PROVIDER = None  # Disabled by default

# Qwen tertiary fallback (DISABLED BY DEFAULT)
ENABLE_QWEN_FALLBACK = os.getenv("ENABLE_QWEN_FALLBACK", "false").lower() == "true"
if ENABLE_QWEN_FALLBACK:
    TERTIARY_PROVIDER = "qwen"
```

**Exported in:** `config/__init__.py`

---

### 2. **Gemini Handling** ‚úÖ

**File:** `orchestrator/llm_client.py` - `GeminiClient`

- Uses Gemini as primary provider
- Detects quota exhaustion via `QuotaExceededError`
- Automatically fallbacks to Groq (no retry on Gemini)
- Logs clear fallback reason

---

### 3. **Groq Handling (CRITICAL - 8B ONLY)** ‚úÖ

**File:** `orchestrator/llm_client.py` - `GroqClient`

```python
class GroqClient(LLMClient):
    ALLOWED_MODELS = [
        "groq/llama-3.1-8b-instant",
        "groq/llama-3.2-8b-instant",
        "groq/llama3-8b-8192"
    ]
    
    def __init__(self, model: str = "groq/llama-3.1-8b-instant", verbose: bool = VERBOSE):
        # CRITICAL: Hard fail on 70B models
        if model not in self.ALLOWED_MODELS:
            raise ConfigurationError(
                f"üõë FORBIDDEN: Groq model '{model}' is not allowed!\n"
                f"   Only 8B models permitted: {self.ALLOWED_MODELS}\n"
                f"   70B models exhaust TPD quota and crash demos.\n"
                f"   Use: groq/llama-3.1-8b-instant"
            )
```

**Validation at startup:** `config/settings.py`

```python
GROQ_ALLOWED_MODELS = [
    "llama-3.1-8b-instant",
    "llama-3.2-8b-instant",
    "llama3-8b-8192"
]
GROQ_FALLBACK_MODEL = os.getenv("GROQ_FALLBACK_MODEL", "llama-3.1-8b-instant")

# Validate Groq model at startup
if GROQ_FALLBACK_MODEL not in GROQ_ALLOWED_MODELS:
    raise ConfigurationError(
        f"‚ùå FORBIDDEN: Groq model '{GROQ_FALLBACK_MODEL}' is not allowed!\n"
        f"   Only 8B models permitted: {GROQ_ALLOWED_MODELS}\n"
        f"   70B models will exhaust TPD quota.\n"
        f"   Update GROQ_FALLBACK_MODEL in .env to: llama-3.1-8b-instant"
    )
```

**Result:** System will **CRASH AT STARTUP** if 70B model configured ‚úì

---

### 4. **Qwen Handling (DISABLED BY DEFAULT)** ‚úÖ

**File:** `.env`

```env
# Enable Qwen tertiary fallback (true/false - DEFAULT: false)
# Only used when BOTH Gemini AND Groq have failed
# Requires valid QWEN_API_KEY to be set
ENABLE_QWEN_FALLBACK=false
```

**File:** `orchestrator/llm_client.py` - `MultiProviderLLM`

```python
def __init__(self, ...):
    # Initialize tertiary client ONLY if enabled
    self.qwen = None
    self.tertiary_enabled = ENABLE_QWEN_FALLBACK and tertiary is not None
    
    if self.tertiary_enabled:
        self._log("‚ö†Ô∏è  Qwen tertiary fallback ENABLED")
        self.qwen = QwenClient(model=qwen_model, verbose=verbose)
    else:
        self._log("‚úì Qwen tertiary fallback DISABLED (graceful abort on Groq failure)")
```

**Result:** 
- Qwen NEVER used unless `ENABLE_QWEN_FALLBACK=true` in .env
- System gracefully aborts if Qwen disabled and Groq fails
- No automatic Qwen usage ‚úì

---

### 5. **Graceful Failure (NO HARD CRASH)** ‚úÖ

**File:** `orchestrator/llm_client.py` - `_graceful_abort()`

When all enabled providers fail:

```python
def _graceful_abort(self, primary_reason: str, secondary_reason: str, tertiary_reason: Optional[str] = None):
    """
    Gracefully abort when all enabled providers are exhausted.
    
    Instead of crashing, returns a controlled error that:
    - Preserves reasoning trace with provider attempts
    - Shows clear user message
    - Allows system to fail safely
    """
    error_msg = (
        f"LLM quota temporarily exhausted. All enabled providers unavailable:\n"
        f"  ‚Ä¢ {self.primary_name.title()}: {primary_reason}\n"
        f"  ‚Ä¢ {self.secondary_name.title()}: {secondary_reason}\n\n"
        f"Please wait a few minutes and retry, or enable tertiary fallback (Qwen)."
    )
    
    raise QuotaExceededError(error_msg)
```

**File:** `orchestrator/batch_optimized_orchestrator.py`

```python
except QuotaExceededError as e:
    # LLM quota exhausted across all enabled providers - graceful abort
    self._log("‚ö†Ô∏è LLM QUOTA EXHAUSTED - returning graceful failure")
    
    # Get provider attempt history for reasoning trace
    provider_stats = self.llm.get_stats()
    provider_attempts = provider_stats.get('last_provider_attempts', [])
    
    return self._abort_quota_exhausted(state, str(e), provider_attempts)
```

**Result:**
- ‚úì No hard crash
- ‚úì Clear error message
- ‚úì Partial reasoning trace preserved
- ‚úì No partial SQL generated
- ‚úì Provider attempts visible in trace

---

### 6. **Reasoning Trace (VISIBLE TO JUDGES)** ‚úÖ

**File:** `orchestrator/batch_optimized_orchestrator.py` - `_abort_quota_exhausted()`

```python
def _abort_quota_exhausted(self, state, error_message, provider_attempts):
    """
    Create graceful quota exhaustion response.
    
    Shows:
    - Clear user message about quota exhaustion
    - Provider attempt history in reasoning trace
    - Partial progress preserved
    - No hard crash, no partial SQL
    
    This is VISIBLE to judges in both CLI and UI.
    """
    # Add quota exhaustion info to reasoning trace
    provider_summary = "\n".join([
        f"  ‚Ä¢ {attempt['provider'].title()}: {attempt['status']}" +
        (f" - {attempt['reason']}" if attempt.get('status') == 'failed' else "")
        for attempt in provider_attempts
    ])
    
    trace_actions.append(
        AgentAction(
            agent_name="QuotaManager",
            action="ABORTED_DUE_TO_QUOTA",
            input_summary=f"LLM quota exhausted after {len(provider_attempts)} provider attempts",
            output_summary=f"System gracefully aborted",
            reasoning=(
                f"Provider fallback chain exhausted:\n{provider_summary}\n\n"
                f"All enabled LLM providers are temporarily unavailable. "
                f"Please wait a few minutes for quota reset and retry."
            )
        )
    )
```

**Visible in:**
- ‚úì CLI output
- ‚úì Streamlit UI reasoning trace
- ‚úì Shows all provider attempts
- ‚úì Shows failure reasons per provider

---

### 7. **Configuration Summary**

**File:** `.env`

```env
# Enable Qwen tertiary fallback (true/false - DEFAULT: false)
ENABLE_QWEN_FALLBACK=false

# Which provider to use (groq or gemini)
LLM_PROVIDER=gemini

# Model selection (CRITICAL - DO NOT CHANGE GROQ MODEL)
# - Groq: groq/llama-3.1-8b-instant (ENFORCED - 70B models FORBIDDEN)
# - Gemini: gemini/gemini-2.5-flash or gemini/gemini-1.5-pro
LLM_MODEL=gemini/gemini-2.5-flash
GROQ_FALLBACK_MODEL=llama-3.1-8b-instant
```

---

## Current Fallback Chain

### Default (ENABLE_QWEN_FALLBACK=false):

```
Gemini ‚Üí Groq (8B only) ‚Üí [Graceful Abort]
```

### With Qwen Enabled (ENABLE_QWEN_FALLBACK=true):

```
Gemini ‚Üí Groq (8B only) ‚Üí Qwen (512 tokens max)
```

---

## What Changed

| **Before** | **After** |
|------------|-----------|
| Qwen automatically used | Qwen behind feature flag (disabled by default) |
| Groq used 70B model | **HARD FAIL** if 70B model configured |
| Hard crash on quota exhaustion | Graceful abort with clear message |
| No provider visibility | Provider attempts in reasoning trace |
| Providers dynamically chosen | Single source of truth in `config/settings.py` |

---

## Testing

### 1. Verify Configuration Loads

```bash
python -c "import config; print(f'ENABLE_QWEN_FALLBACK = {config.ENABLE_QWEN_FALLBACK}'); print(f'GROQ_FALLBACK_MODEL = {config.GROQ_FALLBACK_MODEL}')"
```

**Expected Output:**
```
ENABLE_QWEN_FALLBACK = False
GROQ_FALLBACK_MODEL = llama-3.1-8b-instant
```

### 2. Test 70B Model Rejection

Update `.env`:
```env
GROQ_FALLBACK_MODEL=llama-3.1-70b-versatile
```

Run:
```bash
python cli.py
```

**Expected:** System crashes at startup with:
```
‚ùå FORBIDDEN: Groq model 'llama-3.1-70b-versatile' is not allowed!
   Only 8B models permitted: ['llama-3.1-8b-instant', 'llama-3.2-8b-instant', 'llama3-8b-8192']
   70B models will exhaust TPD quota.
   Update GROQ_FALLBACK_MODEL in .env to: llama-3.1-8b-instant
```

### 3. Test Graceful Quota Exhaustion

Exhaust Gemini and Groq quotas, then run:
```bash
python cli.py -q "How many customers?"
```

**Expected:** Graceful error with:
```
LLM quota temporarily exhausted. All enabled providers unavailable:
  ‚Ä¢ Gemini: quota exceeded
  ‚Ä¢ Groq: rate limit hit

Please wait a few minutes and retry, or enable tertiary fallback (Qwen).
```

**Reasoning trace shows:**
- Provider attempts (Gemini failed, Groq failed)
- Clear abort reason
- No partial SQL

---

## Files Modified

1. ‚úÖ `.env` - Added `ENABLE_QWEN_FALLBACK` flag, updated comments
2. ‚úÖ `config/settings.py` - Provider configuration, Groq model validation
3. ‚úÖ `config/__init__.py` - Export new constants
4. ‚úÖ `orchestrator/llm_client.py` - Conditional Qwen, graceful abort, 8B enforcement
5. ‚úÖ `orchestrator/batch_optimized_orchestrator.py` - Quota exhaustion handling

---

## Production Safety Checklist

- [x] Qwen disabled by default
- [x] 70B models cause startup failure
- [x] Graceful abort when quota exhausted
- [x] Provider attempts visible in trace
- [x] No hard crashes
- [x] No partial SQL on failure
- [x] Single source of truth for providers
- [x] Clear error messages for users
- [x] Judges can see fallback chain

---

## Next Steps

1. **Test in demo environment:**
   - Verify Gemini ‚Üí Groq fallback works
   - Verify graceful abort when both fail
   - Check reasoning trace visibility

2. **Optional: Enable Qwen for extra safety:**
   ```env
   ENABLE_QWEN_FALLBACK=true
   ```

3. **Monitor provider usage:**
   - Check logs for fallback indicators
   - Verify 8B model usage on Groq
   - Ensure no 70B calls ever occur
